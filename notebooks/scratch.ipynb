{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa3a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP module path: /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/clip/__init__.py\n",
      "CLIP module contents: ['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'available_models', 'clip', 'load', 'model', 'simple_tokenizer', 'tokenize']\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "print(\"CLIP module path:\", clip.__file__)\n",
    "print(\"CLIP module contents:\", dir(clip))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874dffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__']\n",
      "/Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/clip/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "print(dir(clip))  # should now show `load`, `tokenize`, etc.\n",
    "print(clip.__file__)  # should point to something like openai_clip/clip/__init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d245fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftfy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# make sure it's relative to your app.py location\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCLIP\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clip\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/fashion-recommender/CLIP/clip/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/repos/fashion-recommender/CLIP/clip/clip.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimple_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleTokenizer \u001b[38;5;28;01mas\u001b[39;00m _Tokenizer\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n",
      "File \u001b[0;32m~/repos/fashion-recommender/CLIP/clip/simple_tokenizer.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mftfy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_bpe\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ftfy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")  # make sure it's relative to your app.py location\n",
    "from CLIP import clip\n",
    "\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "print(\"CLIP model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d07d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4d4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fcafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = '/Users/ayeshamendoza/repos/fashion-recommender/data/output'\n",
    "captions_df = pd.read_csv(os.path.join(output_dir, \"mvp_zara_blip_captions_updated.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad67a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_caption_list = captions_df[captions_df[\"image\"] == \"zara_45.jpg\"][\"caption\"].values[0]\n",
    "category_dict = {\n",
    "    \"shirt\": \"top\", \"blouse\": \"top\", \"sweater\":\"top\", \"cardigan\":\"top\", \"vest\":\"top\",\"top\":\"top\",\n",
    "    \"pant\": \"bottom\", \"skirt\": \"bottom\", \"jean\": \"bottom\", \"shorts\": \"bottom\", \"skort\": \"bottom\", \"trousers\":\"bottom\",\n",
    "    \"dress\": \"dress\", \"jumpsuit\": \"jumpsuit\",\n",
    "    \"jacket\":\"jacket\", \"blazer\":\"jacket\", \n",
    "    \"shoe\": \"shoes\", \"sneaker\": \"shoes\", \"boot\": \"shoes\", \"loafer\": \"shoes\", \"heels\": \"shoes\",\"flats\":\"shoes\",\n",
    "    \"sandals\":\"shoes\",\n",
    "    \"bag\": \"accessory\", \"scarf\": \"accessory\", \"hat\": \"accessory\"\n",
    "}\n",
    "\n",
    "def detect_item_type(image_caption):\n",
    "    image_caption = image_caption.lower()\n",
    "    for keyword, category in category_dict.items():\n",
    "        if keyword in image_caption:\n",
    "            return category\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f35909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_type_map = {}\n",
    "for ix, row in captions_df.iterrows():\n",
    "    gallery_type_map[row[\"image\"]] = detect_item_type(row[\"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f0a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zara_01.jpg': 'top',\n",
       " 'zara_02.jpg': 'jumpsuit',\n",
       " 'zara_03.jpg': 'top',\n",
       " 'zara_04.jpg': 'top',\n",
       " 'zara_05.jpg': 'bottom',\n",
       " 'zara_06.jpg': 'jacket',\n",
       " 'zara_07.jpg': 'top',\n",
       " 'zara_08.jpg': 'bottom',\n",
       " 'zara_09.jpg': 'bottom',\n",
       " 'zara_10.jpg': 'bottom',\n",
       " 'zara_11.jpg': 'dress',\n",
       " 'zara_12.jpg': 'dress',\n",
       " 'zara_13.jpg': 'bottom',\n",
       " 'zara_14.jpg': 'top',\n",
       " 'zara_15.jpg': 'top',\n",
       " 'zara_16.jpg': 'top',\n",
       " 'zara_17.jpg': 'top',\n",
       " 'zara_18.jpg': 'bottom',\n",
       " 'zara_19.jpg': 'bottom',\n",
       " 'zara_20.jpg': 'top',\n",
       " 'zara_21.jpg': 'dress',\n",
       " 'zara_22.jpg': 'dress',\n",
       " 'zara_23.jpg': 'dress',\n",
       " 'zara_24.jpg': 'bottom',\n",
       " 'zara_25.jpg': 'dress',\n",
       " 'zara_26.jpg': 'bottom',\n",
       " 'zara_27.jpg': 'top',\n",
       " 'zara_28.jpg': 'top',\n",
       " 'zara_29.jpg': 'bottom',\n",
       " 'zara_30.jpg': 'accessory',\n",
       " 'zara_31.jpg': 'top',\n",
       " 'zara_32.jpg': 'accessory',\n",
       " 'zara_33.jpg': 'top',\n",
       " 'zara_34.jpg': 'top',\n",
       " 'zara_35.jpg': 'top',\n",
       " 'zara_36.jpg': 'top',\n",
       " 'zara_37.jpg': 'top',\n",
       " 'zara_38.jpg': 'bottom',\n",
       " 'zara_39.jpg': 'jacket',\n",
       " 'zara_40.jpg': 'top',\n",
       " 'zara_41.jpg': 'top',\n",
       " 'zara_42.jpg': 'bottom',\n",
       " 'zara_43.jpg': 'bottom',\n",
       " 'zara_44.jpg': 'bottom',\n",
       " 'zara_45.jpg': 'bottom',\n",
       " 'zara_46.jpg': 'top',\n",
       " 'zara_47.jpg': 'top',\n",
       " 'zara_48.jpg': 'top',\n",
       " 'zara_49.jpg': 'shoes',\n",
       " 'zara_50.jpg': 'shoes',\n",
       " 'zara_51.jpg': 'shoes',\n",
       " 'zara_53.jpg': 'top'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f25443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion-recommender-F_HgdxGq-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
