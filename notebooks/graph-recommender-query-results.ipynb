{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3167ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a100a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd48160",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/ayeshamendoza/repos/fashion-recommender/data/output'\n",
    "image_dir = '/Users/ayeshamendoza/repos/fashion-recommender/data/images/zara'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a18ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load CSV\n",
    "clip_df = pd.read_csv(os.path.join(output_dir,\"mvp_clip_image_embeddings.csv\")) \n",
    "clip_embeddings = clip_df.drop(columns='filename').values  # convert to numpy array\n",
    "print(clip_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a4b32",
   "metadata": {},
   "source": [
    "âœ… 2. Normalize for cosine similarity / FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import normalize, fusion_retrieve, faiss_retrieve\n",
    "\n",
    "clip_embeddings_norm = normalize(clip_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd8c10",
   "metadata": {},
   "source": [
    "âœ… 3. Run Fusion Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13610aad",
   "metadata": {},
   "source": [
    "Load graph data and get `item_nodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c34eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[143, 512], edge_index=[2, 566], num_nodes=143, num_item_nodes=52, num_attr_nodes=91, node_type=[143])\n",
      "Node feature shape: torch.Size([143, 512])\n",
      "Edge index shape: torch.Size([2, 566])\n",
      "Node type tensor shape: torch.Size([143])\n",
      "Node types: (tensor([0, 1]), tensor([52, 91]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "graph_path = os.path.join(output_dir,\"mvp_fashion_graph_data.pt\")\n",
    "data = torch.load(graph_path, weights_only=False)\n",
    "\n",
    "# ðŸ“Š Inspect\n",
    "print(data)\n",
    "print(f\"Node feature shape: {data.x.shape}\")\n",
    "print(f\"Edge index shape: {data.edge_index.shape}\")\n",
    "\n",
    "print(\"Node type tensor shape:\", data.node_type.shape)\n",
    "print(\"Node types:\", torch.unique(data.node_type, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4036be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_node_indices = (data.node_type == 0).nonzero(as_tuple=True)[0]\n",
    "item_node_indices = item_node_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdbf3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_embeddings = torch.load(os.path.join(output_dir, \"node_logits.pt\")).detach().cpu()\n",
    "gcn_item_embeddings = gcn_embeddings[item_node_indices]\n",
    "\n",
    "# Convert to numpy if needed\n",
    "gcn_item_embeddings = gcn_item_embeddings.numpy()\n",
    "gcn_embeddings_norm = normalize(gcn_item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da60844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1.000000011920929), (51, 0.9285026898439506), (35, 0.9218568474818171), (27, 0.918270451725304), (33, 0.9150771820937609)]\n"
     ]
    }
   ],
   "source": [
    "query_idx = 5\n",
    "query_clip = clip_embeddings_norm[query_idx]\n",
    "query_gcn = gcn_embeddings_norm[query_idx]\n",
    "\n",
    "top_k = fusion_retrieve(\n",
    "    query_clip, query_gcn,\n",
    "    clip_embeddings_norm, gcn_embeddings_norm,\n",
    "    item_ids=list(range(len(clip_embeddings))),\n",
    "    k=5, alpha=0.7\n",
    ")\n",
    "\n",
    "print(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17b1d5",
   "metadata": {},
   "source": [
    "### Save Top-K Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7619d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_top_k = pd.DataFrame(top_k, columns=[\"item_index\", \"fusion_score\"])\n",
    "df_top_k.to_csv(os.path.join(output_dir, \"top_k_fusion_results.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7709ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_index</th>\n",
       "      <th>fusion_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0.928503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>0.921857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0.918270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0.915077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_index  fusion_score\n",
       "0           5      1.000000\n",
       "1          51      0.928503\n",
       "2          35      0.921857\n",
       "3          27      0.918270\n",
       "4          33      0.915077"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "409ef4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the mapping\n",
    "with open(os.path.join(output_dir, \"item_to_idx.pkl\"), \"rb\") as f:\n",
    "    item_to_idx = pickle.load(f)\n",
    "\n",
    "# Reverse it: idx â†’ item\n",
    "idx_to_item = {v: k for k, v in item_to_idx.items()}\n",
    "sorted_items = [idx_to_item[i] for i in sorted(idx_to_item)]\n",
    "\n",
    "# Save to CSV\n",
    "pd.DataFrame({\"filename\": sorted_items}).to_csv(os.path.join(output_dir,\"item_names.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30172659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zara_01.jpg',\n",
       " 'zara_02.jpg',\n",
       " 'zara_03.jpg',\n",
       " 'zara_04.jpg',\n",
       " 'zara_05.jpg',\n",
       " 'zara_06.jpg',\n",
       " 'zara_07.jpg',\n",
       " 'zara_08.jpg',\n",
       " 'zara_09.jpg',\n",
       " 'zara_10.jpg',\n",
       " 'zara_11.jpg',\n",
       " 'zara_12.jpg',\n",
       " 'zara_13.jpg',\n",
       " 'zara_14.jpg',\n",
       " 'zara_15.jpg',\n",
       " 'zara_16.jpg',\n",
       " 'zara_17.jpg',\n",
       " 'zara_18.jpg',\n",
       " 'zara_19.jpg',\n",
       " 'zara_20.jpg',\n",
       " 'zara_21.jpg',\n",
       " 'zara_22.jpg',\n",
       " 'zara_23.jpg',\n",
       " 'zara_24.jpg',\n",
       " 'zara_25.jpg',\n",
       " 'zara_26.jpg',\n",
       " 'zara_27.jpg',\n",
       " 'zara_28.jpg',\n",
       " 'zara_29.jpg',\n",
       " 'zara_30.jpg',\n",
       " 'zara_31.jpg',\n",
       " 'zara_32.jpg',\n",
       " 'zara_33.jpg',\n",
       " 'zara_34.jpg',\n",
       " 'zara_35.jpg',\n",
       " 'zara_36.jpg',\n",
       " 'zara_37.jpg',\n",
       " 'zara_38.jpg',\n",
       " 'zara_39.jpg',\n",
       " 'zara_40.jpg',\n",
       " 'zara_41.jpg',\n",
       " 'zara_42.jpg',\n",
       " 'zara_43.jpg',\n",
       " 'zara_44.jpg',\n",
       " 'zara_45.jpg',\n",
       " 'zara_46.jpg',\n",
       " 'zara_47.jpg',\n",
       " 'zara_48.jpg',\n",
       " 'zara_49.jpg',\n",
       " 'zara_50.jpg',\n",
       " 'zara_51.jpg',\n",
       " 'zara_53.jpg']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "865851bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zara_01.jpg',\n",
       " 'zara_02.jpg',\n",
       " 'zara_03.jpg',\n",
       " 'zara_04.jpg',\n",
       " 'zara_05.jpg',\n",
       " 'zara_06.jpg',\n",
       " 'zara_07.jpg',\n",
       " 'zara_08.jpg',\n",
       " 'zara_09.jpg',\n",
       " 'zara_10.jpg',\n",
       " 'zara_11.jpg',\n",
       " 'zara_12.jpg',\n",
       " 'zara_13.jpg',\n",
       " 'zara_14.jpg',\n",
       " 'zara_15.jpg',\n",
       " 'zara_16.jpg',\n",
       " 'zara_17.jpg',\n",
       " 'zara_18.jpg',\n",
       " 'zara_19.jpg',\n",
       " 'zara_20.jpg',\n",
       " 'zara_21.jpg',\n",
       " 'zara_22.jpg',\n",
       " 'zara_23.jpg',\n",
       " 'zara_24.jpg',\n",
       " 'zara_25.jpg',\n",
       " 'zara_26.jpg',\n",
       " 'zara_27.jpg',\n",
       " 'zara_28.jpg',\n",
       " 'zara_29.jpg',\n",
       " 'zara_30.jpg',\n",
       " 'zara_31.jpg',\n",
       " 'zara_32.jpg',\n",
       " 'zara_33.jpg',\n",
       " 'zara_34.jpg',\n",
       " 'zara_35.jpg',\n",
       " 'zara_36.jpg',\n",
       " 'zara_37.jpg',\n",
       " 'zara_38.jpg',\n",
       " 'zara_39.jpg',\n",
       " 'zara_40.jpg',\n",
       " 'zara_41.jpg',\n",
       " 'zara_42.jpg',\n",
       " 'zara_43.jpg',\n",
       " 'zara_44.jpg',\n",
       " 'zara_45.jpg',\n",
       " 'zara_46.jpg',\n",
       " 'zara_47.jpg',\n",
       " 'zara_48.jpg',\n",
       " 'zara_49.jpg',\n",
       " 'zara_50.jpg',\n",
       " 'zara_51.jpg',\n",
       " 'zara_53.jpg']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df = pd.read_csv(os.path.join(output_dir,\"item_names.csv\"))\n",
    "list(items_df['filename'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83bd095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_k[\"item_name\"] = [idx_to_item[img_idx] for img_idx in df_top_k[\"item_index\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee4dd2",
   "metadata": {},
   "source": [
    "### Sample `streamlit` visualization code\n",
    "```\n",
    "st.image(\"images/\" + item_name)\n",
    "st.text(f\"Score: {score}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8f866",
   "metadata": {},
   "source": [
    "FAISS Retrieval \n",
    "\n",
    "- Normalize CLIP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "073f1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_embeddings_norm\n",
    "\n",
    "from retrieval import faiss_retrieve\n",
    "\n",
    "query_clip = clip_embeddings_norm[query_idx]\n",
    "top_k_faiss = faiss_retrieve(query_clip, clip_embeddings_norm, list(range(len(clip_embeddings_norm))), k=5)\n",
    "\n",
    "df_faiss = pd.DataFrame(top_k_faiss, columns=[\"item_index\", \"faiss_score\"])\n",
    "df_faiss.to_csv(os.path.join(output_dir,\"top_k_faiss_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 13:08:08.106 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-16 13:08:08.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Runtime hasn't been created!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/elements/lib/image_utils.py:286\u001b[0m, in \u001b[0;36mimage_to_url\u001b[0;34m(image, width, clamp, channels, output_format, image_id)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    287\u001b[0m         image_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ayeshamendoza/repos/fashion-recommender/data/output/zara_06.jpg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m top_k \u001b[38;5;241m=\u001b[39m fusion_retrieve(query_clip, query_gcn, clip_embeddings_norm, gcn_embeddings_norm, item_ids, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, alpha\u001b[38;5;241m=\u001b[39malpha)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_id, score \u001b[38;5;129;01min\u001b[39;00m top_k:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43midx_to_item\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     st\u001b[38;5;241m.\u001b[39mcaption(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFusion Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/runtime/metrics_util.py:410\u001b[0m, in \u001b[0;36mgather_metrics.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to collect command telemetry\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39mex)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnon_optional_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RerunException \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# Duplicated from below, because static analysis tools get confused\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# by deferring the rethrow.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracking_activated \u001b[38;5;129;01mand\u001b[39;00m command_telemetry:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/elements/image.py:181\u001b[0m, in \u001b[0;36mImageMixin.image\u001b[0;34m(self, image, caption, width, use_column_width, clamp, channels, output_format, use_container_width)\u001b[0m\n\u001b[1;32m    178\u001b[0m         image_width \u001b[38;5;241m=\u001b[39m WidthBehavior\u001b[38;5;241m.\u001b[39mMIN_IMAGE_OR_CONTAINER\n\u001b[1;32m    180\u001b[0m image_list_proto \u001b[38;5;241m=\u001b[39m ImageListProto()\n\u001b[0;32m--> 181\u001b[0m \u001b[43mmarshall_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_delta_path_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_list_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdg\u001b[38;5;241m.\u001b[39m_enqueue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_list_proto)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/elements/lib/image_utils.py:442\u001b[0m, in \u001b[0;36mmarshall_images\u001b[0;34m(coordinates, image, caption, width, proto_imgs, clamp, channels, output_format)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# We use the index of the image in the input image list to identify this image inside\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# MediaFileManager. For this, we just add the index to the image's \"coordinates\".\u001b[39;00m\n\u001b[1;32m    440\u001b[0m image_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (coordinates, coord_suffix)\n\u001b[0;32m--> 442\u001b[0m proto_img\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_id\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/elements/lib/image_utils.py:298\u001b[0m, in \u001b[0;36mimage_to_url\u001b[0;34m(image, width, clamp, channels, output_format, image_id)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     mimetype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/octet-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmedia_file_mgr\u001b[38;5;241m.\u001b[39madd(image, mimetype, image_id)\n\u001b[1;32m    299\u001b[0m caching\u001b[38;5;241m.\u001b[39msave_media_data(image, mimetype, image_id)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/runtime/__init__.py:28\u001b[0m, in \u001b[0;36mget_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_instance\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runtime:\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the singleton Runtime instance. Raise an Error if the\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Runtime hasn't been created yet.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRuntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/fashion-recommender-F_HgdxGq-py3.10/lib/python3.10/site-packages/streamlit/runtime/runtime.py:163\u001b[0m, in \u001b[0;36mRuntime.instance\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the singleton Runtime instance. Raise an Error if the\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mRuntime hasn't been created yet.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime hasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt been created!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Runtime hasn't been created!"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "query_idx = st.slider(\"Select an item index\", 0, len(clip_embeddings)-1, 5)\n",
    "alpha = st.slider(\"Blend weight (visual vs graph)\", 0.0, 1.0, 0.7)\n",
    "\n",
    "query_clip = clip_embeddings_norm[query_idx]\n",
    "query_gcn  = gcn_embeddings_norm[query_idx]\n",
    "\n",
    "top_k = fusion_retrieve(query_clip, query_gcn, clip_embeddings_norm, gcn_embeddings_norm, item_ids, k=5, alpha=alpha)\n",
    "\n",
    "for item_id, score in top_k:\n",
    "    st.image(os.path.join(output_dir,idx_to_item[item_id]))\n",
    "    st.caption(f\"Fusion Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d1cbc",
   "metadata": {},
   "source": [
    "Compare Fusion vs FAISS Results Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36fd7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both to DataFrames\n",
    "df_fusion = pd.DataFrame(top_k, columns=[\"item_index\", \"fusion_score\"])\n",
    "df_faiss = pd.DataFrame(top_k_faiss, columns=[\"item_index\", \"faiss_score\"])\n",
    "\n",
    "# Merge on item_index\n",
    "df_compare = pd.merge(df_fusion, df_faiss, on=\"item_index\", how=\"outer\").sort_values(by=\"fusion_score\", ascending=False)\n",
    "\n",
    "# (Optional) Add item names\n",
    "df_compare[\"item_name\"] = [idx_to_item[i] for i in df_compare[\"item_index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae18894a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_index</th>\n",
       "      <th>fusion_score</th>\n",
       "      <th>faiss_score</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>zara_06.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0.928503</td>\n",
       "      <td>0.897866</td>\n",
       "      <td>zara_53.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>0.921857</td>\n",
       "      <td>0.888373</td>\n",
       "      <td>zara_36.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>0.918270</td>\n",
       "      <td>0.883251</td>\n",
       "      <td>zara_28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0.915077</td>\n",
       "      <td>0.878722</td>\n",
       "      <td>zara_34.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_index  fusion_score  faiss_score    item_name\n",
       "0           5      1.000000     1.000000  zara_06.jpg\n",
       "4          51      0.928503     0.897866  zara_53.jpg\n",
       "3          35      0.921857     0.888373  zara_36.jpg\n",
       "1          27      0.918270     0.883251  zara_28.jpg\n",
       "2          33      0.915077     0.878722  zara_34.jpg"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af8997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion-recommender-F_HgdxGq-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
